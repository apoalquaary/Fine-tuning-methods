{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ef3820",
   "metadata": {},
   "source": [
    "# Setup the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e48ce3",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e03a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda9a84",
   "metadata": {},
   "source": [
    "## Calling a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"\" # Model Name or Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map='auto',\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e06779f",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "    You are an assistant, Answer the user about anything. \n",
    "    <</SYS>>\n",
    "    \"It was a bad week, what should I plan for the weekend?\"[/INST]\"\"\"\n",
    "\n",
    "batch = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "  output_tokens = model.generate(**batch, max_new_tokens=250, repetition_penalty=1.1,)\n",
    "\n",
    "output = tokenizer.decode(output_tokens[0], skip_special_tokens=True).replace(prompt, \"\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd02eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "    You are a key word finder, Find the describing words for the following sentence \n",
    "    <</SYS>>\n",
    "    \"After they had break up, he wasn't the same\"[/INST]\"\"\"\n",
    "\n",
    "batch = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "output_tokens = model.generate(**batch, max_new_tokens=50, repetition_penalty=1.1,)\n",
    "\n",
    "output = tokenizer.decode(output_tokens[0], skip_special_tokens=True).replace(prompt, \"\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54989f31",
   "metadata": {},
   "source": [
    "### Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d554e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        print(param.shape, param.numel(), param.dtype)\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ed116",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4bf4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf5a03",
   "metadata": {},
   "source": [
    "### Lora Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = LoraConfig(\n",
    "    r=4, # rank\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    target_modules=['q_proj','k_proj', \"v_proj\", \"o_proj\"], #if you know the \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "print_trainable_parameters(peft_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193a4ce0",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14726972",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"Abirate/english_quotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ee823",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3725a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(example):\n",
    "    example['prediction'] = f\"\"\"<s>[INST] <<SYS>>\n",
    "    You are a key word finder, Find the describing words for the following sentence \n",
    "    <</SYS>>\n",
    "    {example[\"quote\"]}[/INST]\n",
    "    {str(example[\"tags\"])} </s>\"\"\"\n",
    "    return example\n",
    "\n",
    "data['train'] = data['train'].map(merge_columns)\n",
    "data['train'][\"prediction\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824cc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda sample: tokenizer(sample['prediction']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb447d",
   "metadata": {},
   "source": [
    "### If you are using llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = \"[PAD]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689beafc",
   "metadata": {},
   "source": [
    "### Fine-tunning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f482de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=peft_model, \n",
    "    train_dataset=data['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1, \n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=100, \n",
    "        max_steps=100, \n",
    "        learning_rate=2e-4, \n",
    "        fp16=True,\n",
    "        logging_steps=1, \n",
    "        output_dir='outputs',\n",
    "\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "peft_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "    You are a key word finder, Find the describing words for the following sentence \n",
    "    <</SYS>>\n",
    "    \"You lost in the finals\"[/INST]\"\"\"\n",
    "\n",
    "batch = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "  output_tokens = peft_model.generate(**batch, max_new_tokens=50, repetition_penalty=1.1,)\n",
    "\n",
    "output = tokenizer.decode(output_tokens[0], skip_special_tokens=True).replace(prompt, '')\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a2c95",
   "metadata": {},
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e486b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"llama-2-7b-chat-hf-lora\")\n",
    "\n",
    "# lora_model.push_to_hub(\"your-name/llama-2-7b-chat-hf-lora\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c272843",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b24613",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(\"llama-2-7b-chat-hf-lora\")\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "lora_model = PeftModel.from_pretrained(model, \"llama-2-7b-chat-hf-lora\") # is_trainable=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "    You are a key word finder, Find the describing words for the following sentence \n",
    "    <</SYS>>\n",
    "    \"He is a good player.\"[/INST]\"\"\"\n",
    "\n",
    "batch = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "  output_tokens = lora_model.generate(**batch, max_new_tokens=50, repetition_penalty=1.1,)\n",
    "\n",
    "output = tokenizer.decode(output_tokens[0], skip_special_tokens=True).replace(prompt, '').lstrip()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051b84e",
   "metadata": {},
   "source": [
    "### Merge Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861496a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = lora_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"Merged_Model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
